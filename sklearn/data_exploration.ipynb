{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Data Exploration\n",
    "#### This notebook aims to find out useful features \n",
    "### <font color=blue> Rules :\n",
    "#### Since we are a lot contributing to the notebook, please comment your code and use explicit variable names\n",
    "#### Do not modify the raw data as others would like to work on it\n",
    "#### Only modify your section (copy-paste others code if needed) so that no clashes occur when pulling/pushing on remote repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>Hugo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specific imports in your section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>Nicolas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specific imports in your section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>Mohamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import nltk\n",
    "from tokenizer import Tokenizer\n",
    "import seaborn as sns\n",
    "import time\n",
    "import sys\n",
    "from scipy import sparse\n",
    "sys.path.append('../text_mining')\n",
    "sys.path.append('../classifier')\n",
    "from n_gram_creation import vectorize_n_grams\n",
    "from tfidfvectorizer import TfidfVectorizer\n",
    "import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fname = 'train.csv'\n",
    "#test_fname = 'test.csv'\n",
    "X = pd.read_csv(train_fname,header=None)\n",
    "y = pd.read_csv(train_fname, header=None)[0]\n",
    "#X_test = pd.read_csv(test_fname, header=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.columns = ['label', 'content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3063\n",
       "1    1352\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 8, 11, 12, 15, 18, 23, 27, 28, 29]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_1 = [index for index in range(X.shape[0]) if y[index]==1]\n",
    "index_1[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some insulting comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     \"who told you that....stupid ask yourself who ...\n",
       "8                             \"You're an idiot baldo. \"\n",
       "11    \"You show your stupidity and ignorance each an...\n",
       "12    \"Wait until you are a minority in your own red...\n",
       "15    \"Its \"faggot\" you ignorant fuck plus what you ...\n",
       "18     \"Get thatdick outta your mouth so you can talk.\"\n",
       "23    \"Judging by your prior posts listed in your pr...\n",
       "27    \"RYAN...........GO BACK TO SUCKING YOUR MOTHER...\n",
       "28                                  \"Your are an idiot\"\n",
       "29    \"Very sorry I mentioned it.\\xa0 Just keep bitc...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.content[index_1[:10]] # Wow violent lui ... hahaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tokenizing Stemming and Cleaning a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To Keep updated\n",
    "TOKENS = [\n",
    "    ### Replacements first ###\n",
    "    ('BREAK'        , r\"\\\\n\", r\" \"),\n",
    "    ('SLASH'           , r\"\\/+\", r\" \"),\n",
    "    ('WHITE_SPACE'     , r\"(\\\\\\\\xc2|\\\\\\\\xa0|\\\\xa0|\\\\xa1|\\\\xa3|\\\\xa9|\\\\r|\\\\ufeff|\\\\u2013|\\\\u2016|\\\\u200f|\\\\u2665|\\\\U0001f308|\\\\U0001f3e9|\\\\U0001f48b|\\\\\\\\|\\\\|&nbsp|&amp|=|\\\"\\\"|\\\"|\\\\u2018|\\\\u2019)\", r\" \"),\n",
    "    ('LETTER_A'        , r\"(\\\\xe1|\\\\xe3|\\\\xe0|\\\\xc2|\\\\u1ef1|\\\\u0105|\\\\u1eb7|\\\\xe5|\\\\xe4)\", r\"a\"),\n",
    "    ('LETTER_E'        , r\"(\\\\xe9|\\\\xe8|\\\\u1ec3|\\\\u1ec5|\\\\u1ebf|\\\\u0119)\", r\"e\"),\n",
    "    ('LETTER_I'        , r\"(\\\\xed)\", r\"i\"),\n",
    "    ('LETTER_O'        , r\"(\\\\x00|\\\\xf8|\\\\xf6|\\\\xf3)\", r\"o\"),\n",
    "    ('LETTER_U'        , r\"(\\\\xfa|\\\\xdc|\\\\xfc|\\\\u1ee9|\\\\u0169)\", r\"u\"),\n",
    "    ('LETTER_Y'        , r\"(\\\\xfd)\", r\"u\"),\n",
    "    ('LETTER_C'        , r\"(\\\\xe7|\\\\u0107)\", r\"c\"),\n",
    "    ('LETTER_Z'        , r\"(\\\\u017a|\\\\u017e|\\\\u017c)\", r\"z\"),\n",
    "    ('PONCT_!'        , r\"(\\\\u203d)\", r\"!\"),\n",
    "    ### Tokens ###\n",
    "    ('NOT'             , r\"n\\'t\", r\" not \"),\n",
    "    ('PUNCT'      , r\"(!+|\\?+|\\.+)\", r\" \\1 \"),\n",
    "    ('NIL'          , r\"\\'|,\", r\" \"),\n",
    "    ('SMILEY'       , r\"(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)|xD|XD\", r\" smiley \"),\n",
    "    ('PUNCT'      , r\"(;+|\\,+|\\)+|\\(+)|\\-|\\_\", r\" \"),\n",
    "    ('ALONE'        , r\"\\s[a-hj-zA-HJ-Z]\\s\", r\" \"), # delete all lone letters except I&i\n",
    "    ('DOTS'         , r\"\\.+\", r\" \"),\n",
    "#    ('QUOTE'      , r\"(\\\"|\\')+(\\w+)(\\\"|\\')+\", \"QUOTE\"), # A améliorer pour mettre la citation entre deux <QUOTE>\n",
    "    ('BEGIN_WHITESPACE' , r\"^\\s+\", r\"\"),\n",
    "    ('END_WHITESPACE' , r\"\\s+$\", r\"\"),\n",
    "    ('WHITESPACES' , r\"\\s+\", r\" \")\n",
    "]\n",
    "\n",
    "# Ca déconne\n",
    "INSULTS = [\n",
    "    ### Insults Replacment ###\n",
    "    ('FUCK'    , r\"f(.ck|u.k|uc.)(i?ng*)?\", r\" fuck \"),# fuck/fucking style\n",
    "    ('FUCK_2'    , r\"fuck((\\s)?you|.?er)+\", r\" fuck you \"),#fuck you + fuck your XXXer\n",
    "    ('FUCK_3'    , r\"(\\w?-)fucker\", r\" mother fuck \"),# compouned fuck # mother fuck plutot ?\n",
    "    # Replace above regex \"mother fuck\" by \"you fuck\" since personal insult ?\n",
    "    ('ASSHOLE',  r\"a.{1,3}oles?\", r\" asshole \"),# ass hole and its versions\n",
    "    ('STALK',  r\"stalk(in*g*|ers?)?\", r\" stalk \"),\n",
    "    ('STUPID',  r\"(\\$|s)tupi?d\", r\" stupid \"),\n",
    "    ('SHIT',  r\"sh(itty|it)\", r\" shit \"),\n",
    "    ('NIGGA',  r\" nigg | negro \", r\" nigga \"),\n",
    "    ('BEGIN_WHITESPACE' , r\"^\\s+\", r\"\"),\n",
    "    ('END_WHITESPACE' , r\"\\s+$\", r\"\"),\n",
    "    ('WHITESPACES' , r\"\\s+\", r\" \")\n",
    "    \n",
    "    ]\n",
    "\n",
    "RANDOM = [\n",
    "    ### Random Replacement ###\n",
    "    ('LAUGH',  r\"\\b(?:a*(?:ha)+h?|(?:l+o+)+l+)\\b\", r\" laugh \"), # matches laughs such as \"hahaha\" or \"lolll\"\n",
    "    ('PSEUDO',  r\"@\\s?\\w+\", r\" pseudo \"),\n",
    "    ('DOLLAR',  r\"[0-9]+k?\\s?\\$+\", r\" dollar \"),\n",
    "    ('DATE',  r\"[0-9]+th(\\s?of)?(\\s\\w+)?\", r\" date \"),# eg : 18th or \"18th of Month\"\n",
    "    ('YEAR',  r\"[0-9]{4}?\", r\" year \"),\n",
    "    ('NUMBER',  r\"[0-9]{1,}\", r\" number \"),\n",
    "    ('LOVE',  r\"<3\", r\" love \"),\n",
    "    ('URL',  r\"(http:)?(\\/\\/)?((www\\.)?\\w+\\.\\w+)\\/?(\\w+.\\w+)?\", r\" url \"),\n",
    "    ('BEGIN_WHITESPACE' , r\"^\\s+\", r\"\"),\n",
    "    ('END_WHITESPACE' , r\"\\s+$\", r\"\"),\n",
    "    ('WHITESPACES' , r\"\\s+\", r\" \")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myTokenizer = Tokenizer(TOKENS)\n",
    "myReplacer = Tokenizer(RANDOM)\n",
    "myReplacer2 = Tokenizer(INSULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Stick to Santorum.  Your obsession with his wife makes you look like an ass.\\\\n\\\\nThere is plenty to pick on with him.  He is as big a freak as you are\"\n",
      "stick to santorum your obsess with his wife make you look like an ass there is plenti to pick on with him he is as big freak as you are\n"
     ]
    }
   ],
   "source": [
    "input = X.content.values[1055]\n",
    "print(input)\n",
    "tokens = myTokenizer.tokenize(input)\n",
    "new_string = myTokenizer.reconstruct(tokens)\n",
    "output = myReplacer.tokenize(new_string)\n",
    "new_string = myReplacer.reconstruct(output)\n",
    "output = myReplacer2.tokenize(new_string)\n",
    "print(myReplacer2.reconstruct(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Stemming and Cleaning the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_list = X.content.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_X_list = []\n",
    "for input in X_list:\n",
    "    tokens = myTokenizer.tokenize(input)\n",
    "    new_string = myTokenizer.reconstruct(tokens)\n",
    "    output = myReplacer.tokenize(new_string)\n",
    "    new_string = myReplacer.reconstruct(output)\n",
    "    output = myReplacer2.tokenize(new_string)\n",
    "    new_string = myReplacer2.reconstruct(output)\n",
    "    new_X_list.append(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(X_list) == len(new_X_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_new = np.asarray(new_X_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4415,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X['processed'] = X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Imagine being able say, you know what, no san...</td>\n",
       "      <td>imagin being abl say you know what no sanction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>\"\"But Jack from Raleigh wasn't done. He came b...</td>\n",
       "      <td>but jack from raleigh was not done he came bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>\"the Star box allows you to link your comment ...</td>\n",
       "      <td>the star box allow you to link your comment to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Cheney,Rush,Nugent.The list is endless.\"</td>\n",
       "      <td>cheney rush nugent the list is endless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Obama....I'm blow'n mo smoke up yo arses........</td>\n",
       "      <td>obama i blow mo smoke up yo ar anyon wonder wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            content  \\\n",
       "0      0  \"Imagine being able say, you know what, no san...   \n",
       "1      0  \"\"But Jack from Raleigh wasn't done. He came b...   \n",
       "2      0  \"the Star box allows you to link your comment ...   \n",
       "3      0          \"Cheney,Rush,Nugent.The list is endless.\"   \n",
       "4      0  \"Obama....I'm blow'n mo smoke up yo arses........   \n",
       "\n",
       "                                           processed  \n",
       "0  imagin being abl say you know what no sanction...  \n",
       "1  but jack from raleigh was not done he came bac...  \n",
       "2  the star box allow you to link your comment to...  \n",
       "3             cheney rush nugent the list is endless  \n",
       "4  obama i blow mo smoke up yo ar anyon wonder wh...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On définit nos X_train et X_test pour tester nos clssifieurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X.processed, X.label, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,4), analyzer=\"char\")\n",
    "X_train_counts = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_mycount = vectorize_n_grams(X_train, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_tf = TfidfVectorizer()\n",
    "X_train_mytf = my_tf.transform(X_train_mycount[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2649, 105496)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 5]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "params = {'C':[0.001, 0.01, 0.1, 1, 5]}\n",
    "grid = GridSearchCV(logreg, params, cv=3)\n",
    "grid.fit(X_train_mytf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 0.001}, 0.80067950169875424)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = red> Avec notre TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>On génère les n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bigrams\n",
    "X_mycount_2, X_dico_2 = vectorize_n_grams(X.processed,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trigrams\n",
    "X_mycount_3, X_dico_3 = vectorize_n_grams(X.processed,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214.192473\n"
     ]
    }
   ],
   "source": [
    "# Quadrigrams\n",
    "t0 = time.clock()\n",
    "X_mycount_4, X_dico_4 = vectorize_n_grams(X.processed,4)\n",
    "print(time.clock()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4415, 970), (4415, 8834), (4415, 42622))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taille des matrices\n",
    "X_mycount_2.shape, X_mycount_3.shape, X_mycount_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ef', 'nb', 'cy', ..., 'pha', 'hyd', 'ja'], \n",
       "      dtype='<U3')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((X_dico_2,X_dico_3),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52426,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On crée un unique dico\n",
    "dico_global = np.concatenate((X_dico_2,X_dico_3),axis=0)\n",
    "dico_global = np.concatenate((dico_global,X_dico_4),axis=0)\n",
    "dico_global.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0,  0,  0, ...,  3,  1,  0],\n",
       "        [ 0,  0,  0, ...,  0,  2,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ..., \n",
       "        [ 1,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 4,  0,  0, ..., 12,  0,  0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mycount_2.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On crée la matrice des counts globale\n",
    "tmp = np.concatenate((X_mycount_2.todense(), X_mycount_3.todense()), axis=1)\n",
    "X_new = np.concatenate((tmp, X_mycount_4.todense()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4415, 52426)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 1, 0],\n",
       "        [0, 0, 0, ..., 0, 2, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [4, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_new_sparse = sparse.csr_matrix(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4415x52426 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2697730 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> On fait la TFIDF sur cette matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_tfvectorizer = TfidfVectorizer()\n",
    "X_tf = my_tfvectorizer.transform(X_new_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          3.61363962,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          7.22727924,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ..., \n",
       "        [ 2.45522691,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 9.82090763,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = green> On fait notre regression sur cette TFIDF des n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_logreg = LogisticRegression.LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On définit notre test et notre train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_tf, X.label, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2649,) and (1,2649) not aligned: 2649 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-c0b5774c9715>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_logreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/mohamedmf/repos/sd207/classifier/LogisticRegression.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Newton'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'i2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error of penalisation for the solver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mohamedmf/repos/sd207/classifier/LogisticRegression.py\u001b[0m in \u001b[0;36mnewton\u001b[0;34m(x0, X, y, epsilon)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnewton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mxk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mhessienne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhessienne_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mohamedmf/repos/sd207/classifier/LogisticRegression.py\u001b[0m in \u001b[0;36mgrad_f\u001b[0;34m(w2, X, y)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mohamedmf/anaconda/lib/python3.5/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right, name, na_op)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 \u001b[0mlvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m             return left._constructor(wrap_results(na_op(lvalues, rvalues)),\n\u001b[0m\u001b[1;32m    619\u001b[0m                                      \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                                      dtype=dtype)\n",
      "\u001b[0;32m/Users/mohamedmf/anaconda/lib/python3.5/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             result = expressions.evaluate(op, str_rep, x, y,\n\u001b[0;32m--> 553\u001b[0;31m                                           raise_on_error=True, **eval_kwargs)\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mohamedmf/anaconda/lib/python3.5/site-packages/pandas/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, raise_on_error, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         return _evaluate(op, op_str, a, b, raise_on_error=raise_on_error,\n\u001b[0;32m--> 218\u001b[0;31m                          **eval_kwargs)\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_on_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mohamedmf/anaconda/lib/python3.5/site-packages/pandas/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b, raise_on_error, truediv, reversed, **eval_kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mohamedmf/anaconda/lib/python3.5/site-packages/pandas/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b, raise_on_error, **eval_kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_TEST_MODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mohamedmf/anaconda/lib/python3.5/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__rmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rmul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__imul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2649,) and (1,2649) not aligned: 2649 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "my_logreg.fit(X_train.todense(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LogisticRegression())\n",
    "                    ])\n",
    "                    \n",
    "_ = text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.96      0.87      1211\n",
      "          1       0.83      0.45      0.58       555\n",
      "\n",
      "avg / total       0.80      0.80      0.78      1766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE-BAYES : ACCURACY = 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION = 0.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adding general Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addind additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BadWordCounter():\n",
    "    def __init__(self):\n",
    "        with open(\"my_badlist.txt\") as f:\n",
    "            badwords = [l.strip() for l in f.readlines()]\n",
    "        self.badwords_ = badwords\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return np.array(['n_words', 'n_chars', 'allcaps', 'max_len',\n",
    "            'mean_len', '@', '!', 'spaces', 'bad_ratio', 'n_bad',\n",
    "            'capsratio'])\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        ## some handcrafted features!\n",
    "        n_words = [len(c.split()) for c in documents]\n",
    "        n_chars = [len(c) for c in documents]\n",
    "        # number of uppercase words\n",
    "        allcaps = [np.sum([w.isupper() for w in comment.split()])\n",
    "               for comment in documents]\n",
    "        # longest word\n",
    "        max_word_len = [np.max([len(w) for w in c.split()]) for c in documents]\n",
    "        # average word length\n",
    "        mean_word_len = [np.mean([len(w) for w in c.split()])\n",
    "                                            for c in documents]\n",
    "        # number of google badwords:\n",
    "        n_bad = [np.sum([c.lower().count(w) for w in self.badwords_])\n",
    "                                                for c in documents]\n",
    "        exclamation = [c.count(\"!\") for c in documents]\n",
    "        addressing = [c.count(\"@\") for c in documents]\n",
    "        spaces = [c.count(\" \") for c in documents]\n",
    "\n",
    "        allcaps_ratio = np.array(allcaps) / np.array(n_words, dtype=np.float)\n",
    "        bad_ratio = np.array(n_bad) / np.array(n_words, dtype=np.float)\n",
    "\n",
    "        return np.array([n_words, n_chars, allcaps, max_word_len,\n",
    "            mean_word_len, exclamation, addressing, spaces, bad_ratio, n_bad,\n",
    "            allcaps_ratio]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_counter = BadWordCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = bad_counter.transform(X.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4415, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(features, y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_1 = SVC(kernel='linear', C=1)\n",
    "clf_2 = LinearRegression()\n",
    "cross_validation.cross_val_score(clf_1, features, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
